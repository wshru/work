{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b9ef019-738f-441c-9ee1-5d5c063c7cd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Flatten\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv(r\"C:\\Users\\Deepti Wandhekar\\Downloads\\imdb_dataset.csv\")\n",
    "\n",
    "# Split the data into input (reviews) and output (sentiment) columns\n",
    "reviews = data[\"review\"]\n",
    "sentiments = data[\"sentiment\"]\n",
    "\n",
    "# Tokenize the text\n",
    "#tokenizer = Tokenizer(): This line initializes an instance of the Tokenizer class.\n",
    "#tokenizer.fit_on_texts(reviews): This line fits the tokenizer on the provided reviews data. This step builds the vocabulary of the tokenizer based on the words present in the reviews. It assigns a unique index to each word in the vocabulary.\n",
    "#sequences = tokenizer.texts_to_sequences(reviews): This line converts the text data into sequences of integers. It takes the reviews data and replaces each word with its corresponding index from the tokenizer's vocabulary. The resulting sequences variable contains lists of integers, where each integer represents a word in the original text.\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "sequences = tokenizer.texts_to_sequences(reviews)\n",
    "\n",
    "# Pad sequences to a fixed length. pad_sequences function is used to pad the sequences of integers (sequences) to ensure they all have the same length. This is often necessary when working with sequential data in machine learning models\n",
    "#X = pad_sequences(sequences, maxlen=max_length): This line applies padding to the sequences using the pad_sequences function. It takes the sequences as input and pads or truncates them to have a length of max_length. The resulting X variable contains the padded sequences.\n",
    "\n",
    "max_length = 250\n",
    "X = pad_sequences(sequences, maxlen=max_length)\n",
    "\n",
    "# Convert sentiments to binary labels (0 for negative, 1 for positive). NumPy array y is created based on a list of sentiments. The sentiment values are converted to binary labels, where a sentiment of \"positive\" is assigned the label 1, and any other sentiment is assigned the label 0.\n",
    "y = np.array([1 if sentiment == 'positive' else 0 for sentiment in sentiments])\n",
    "\n",
    "# Split the data into training and testing sets The random_state parameter ensures reproducibility by setting a seed value for the random shuffling of the data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eb99e6f-88ed-47c0-b788-6cecbc9d7c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "625/625 [==============================] - 171s 273ms/step - loss: 0.3451 - accuracy: 0.8388 - val_loss: 0.2409 - val_accuracy: 0.9000\n",
      "Epoch 2/7\n",
      "625/625 [==============================] - 166s 265ms/step - loss: 0.0421 - accuracy: 0.9864 - val_loss: 0.3600 - val_accuracy: 0.8740\n",
      "Epoch 3/7\n",
      "625/625 [==============================] - 160s 257ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.4178 - val_accuracy: 0.8818\n",
      "Epoch 4/7\n",
      "625/625 [==============================] - 159s 255ms/step - loss: 2.1724e-04 - accuracy: 1.0000 - val_loss: 0.4294 - val_accuracy: 0.8845\n",
      "Epoch 5/7\n",
      "625/625 [==============================] - 160s 256ms/step - loss: 8.8525e-05 - accuracy: 1.0000 - val_loss: 0.4438 - val_accuracy: 0.8847\n",
      "Epoch 6/7\n",
      "625/625 [==============================] - 166s 266ms/step - loss: 5.1744e-05 - accuracy: 1.0000 - val_loss: 0.4560 - val_accuracy: 0.8858\n",
      "Epoch 7/7\n",
      "625/625 [==============================] - 164s 262ms/step - loss: 3.3063e-05 - accuracy: 1.0000 - val_loss: 0.4675 - val_accuracy: 0.8867\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 0.4675 - accuracy: 0.8867\n",
      "Test loss: 0.4675050377845764\n",
      "Test accuracy: 0.8866999745368958\n",
      "1/1 [==============================] - 0s 101ms/step\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=128, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=7, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test loss:\", loss)\n",
    "print(\"Test accuracy:\", accuracy)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e094b55a-4d04-496a-a757-6cef92207aec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected 'else' after 'if' expression (652079100.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [4], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    predicted_sentiment = \"Positive\" if predictions[i]  fgvvvvvvvvvvvvvvvvvvvbg ff >= 0.5 else \"Negative\"\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m expected 'else' after 'if' expression\n"
     ]
    }
   ],
   "source": [
    "# Display sample predictions and actual sentiments\n",
    "print(\"Sample Predictions:\\n\")\n",
    "for i in range(5):\n",
    "    predicted_sentiment = \"Positive\" if predictions[i]  fgvvvvvvvvvvvvvvvvvvvbg ff >= 0.5 else \"Negative\"\n",
    "    actual_sentiment = \"Positive\" if y_test[i] == 1 else \"Negative\"\n",
    "    print(\"Predicted Sentiment:\", predicted_sentiment)\n",
    "    print(\"Actual Sentiment:\", actual_sentiment)\n",
    "    review_text = reviews.iloc[i]\n",
    "    print(\"Review Text:\\n\", review_text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2346e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22fa80d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650705f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1122c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b9a3cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cc9a51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b132c5c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7377d39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f087bf81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996701ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b4969b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1c02eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045d79dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89d6b40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b1e9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Dataset:\n",
    "IMDB Dataset-The IMDB dataset is a large collection of movie reviews collected from the IMDB\n",
    "website, which is a popular source of user-generated movie ratings and reviews. The dataset consists of\n",
    "50,000 movie reviews, split into 25,000 reviews for training and 25,000 reviews for testing.\n",
    "\n",
    "Each review is represented as a sequence of words, where each word is represented by an integer index\n",
    "based on its frequency in the dataset. The labels for each review are binary, with 0 indicating a negative\n",
    "review and 1 indicating a positive review.\n",
    "\n",
    "##Classification:\n",
    "The Classification algorithm is a Supervised Learning technique that is used to identify the category of new observations on the basis of training data. In Classification, a program learns from the given dataset or observations and then classifies new observation into a number of classes or groups. Such as, Yes or No, 0 or 1, Spam or Not Spam, cat or dog, etc. \n",
    "Classes can be called as targets/labels or categories.\n",
    "\n",
    "##Binary Classification:\n",
    "Binary classification is a fundamental task in machine learning where the goal is to classify data instances into one of two classes or categories. In binary classification, the target variable or label can take only two possible values, often referred to as positive and negative, 1 and 0, or true and false\n",
    "\n",
    "##Applications:\n",
    "Spam Detection: \n",
    "Fraud Detection: \n",
    "Disease Diagnosis\n",
    "Sentiment Analysis\n",
    "Customer Churn Prediction\n",
    "Credit Scoring\n",
    "Intrusion Detection\n",
    "Fault Diagnosis\n",
    "Image Classification\n",
    "Face Recognition\n",
    "\n",
    "## build model:\n",
    "\n",
    "\n",
    "The Sequential() function is used to initialize a sequential model.\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=128, input_length=max_length)) adds an embedding layer to the model. This layer is responsible for converting the input sequences into dense vectors of fixed size.\n",
    "model.add(Flatten()) flattens the embedded output to a 1-dimensional tensor.\n",
    "model.add(Dense(64, activation='relu')) adds a fully connected layer with 64 units and ReLU activation.\n",
    "model.add(Dense(1, activation='sigmoid')) adds the output layer with a single unit and sigmoid activation.\n",
    "\n",
    "##compile:\n",
    "In the code snippet you provided, the model.compile() function is used to compile the deep neural network (DNN) model in Keras. This function specifies the loss function, optimizer, and optional metrics to be used during the training process.\n",
    "\n",
    "Here's an explanation of the parameters used in model.compile():\n",
    "\n",
    "loss='binary_crossentropy': This parameter specifies the loss function to be used for binary classification problems. Binary cross-entropy is a common loss function used when dealing with binary classification tasks. It measures the difference between the predicted and actual class probabilities.\n",
    "\n",
    "optimizer='adam': This parameter specifies the optimizer algorithm to be used during the training process. In this case, the 'adam' optimizer is used. Adam is a popular optimization algorithm that combines the advantages of both Adaptive Gradient Algorithm (AdaGrad) and Root Mean Square Propagation (RMSProp). It adapts the learning rate during training and efficiently handles sparse gradients.\n",
    "\n",
    "metrics=['accuracy']: This parameter specifies the metric(s) to be used to evaluate the model's performance. In this case, the 'accuracy' metric is used. Accuracy measures the percentage of correctly predicted instances out of the total number of instances. It is a commonly used metric for classification problems.\n",
    "\n",
    "    \n",
    "#Train:\n",
    "model.fit() function is used to train the deep neural network (DNN) model using the training data. This function fits the model to the training data, updating the model's parameters based on the specified configuration.\n",
    "batch_size=64: This parameter specifies the number of samples to be used in each training batch. The training data is divided into batches, and the model's parameters are updated after processing each batch. A batch size of 64 means that 64 samples will be processed at a time.\n",
    "epochs=7: This parameter specifies the number of times the entire training dataset will be iterated over during training. Each iteration over the entire dataset is called an epoch. In this case, the model will be trained for 7 epochs.\n",
    "validation_data=(X_test, y_test): This parameter specifies the validation data to be used during training. The model's performance on the validation data will be evaluated after each epoch. The validation data is used to monitor the model's generalization and prevent overfitting.\n",
    "    \n",
    "Make prediction:     \n",
    "In the code snippet you provided, the model.predict() function is used to make predictions using the trained deep neural network (DNN) model. This function takes input data and returns the predicted outputs.\n",
    "Here's an explanation of the code:\n",
    "X_test[:5]: This is a slicing operation that selects the first five examples from the testing data (X_test).\n",
    "    \n",
    "    \n",
    "\n",
    "    ##\n",
    "    pandas (as pd): Used for data manipulation and analysis.\n",
    "numpy (as np): Used for numerical operations and array manipulation.\n",
    "tensorflow (as keras): A popular deep learning framework.\n",
    "Sequential: A class from keras.models module that allows you to create a sequential model by stacking layers.\n",
    "Embedding: A layer from keras.layers module that performs word embedding, commonly used for text data processing.\n",
    "Dense: A layer from keras.layers module that represents a fully connected layer in a neural network.\n",
    "Flatten: A layer from keras.layers module that flattens the input tensor into a 1-dimensional array.\n",
    "Tokenizer: A class from keras.preprocessing.text module that is used to tokenize text data into sequences of integers.\n",
    "pad_sequences: A function from keras.preprocessing.sequence module that pads sequences to a fixed length.\n",
    "Additionally, the train_test_split function from sklearn.model_selection module is imported. This function is used to split the dataset into training and testing sets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb40670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fda382b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
